TITLE: 
Nearshore Wave Prediction System (NWPS) v1.1.0

CHANGE REQUEST CLASS: 
Major change

DESCRIPTION OF CHANGE:  
Expansion of NWPS by 13 coastal WFO domains (Western, Pacific and Alaska Regions) to achieve national coverage of 36 domains alongside existing Southern and Eastern Region coastal WFOs. Expansion of extraction scripts for upstream models ESTOFS and RTOFS to include Pacific basin regions. Addition of the extraction of sea ice fields (Alaska Region). Increase of spectral resultion in overall CG1 grids from 15 deg (24 directional bins) to 10 deg (36 bins) for improved swell propagation. Added empirical rip current guidance output to WFO San Diego domain. Ported system from WCOSS Phase 1 to WCOSS TO4 (Cray).

The initial implementation of NWPS (v1.0.0) included 23 coastal WFO domains in Southern and Eastern Regions:

   BRO - Brownsville, Texas
   CRP - Corpus Christi, Texas
   HGX - Houston/Galveston, Texas
   LCH - Lake Charles, Louisiana
   LIX - New Orleans/Baton Rouge, Louisiana
   MOB - Mobile/Pensacola, Alabama/Florida
   TAE - Tallahassee, Florida
   TBW - Tampa Bay, Florida
   MFL - Miami-South Florida, Florida
   KEY - Key West, Florida
   MLB - Melbourne, Florida
   JAX - Jacksonville, Florida
   SJU - San Juan, Puerto Rico
   CAR - Caribou, Maine
   GYX - Gray/Maine, Maine
   BOX - Boston, Massachusetts
   NYC - New York, New York
   PHI - Mt. Holly, New Jersey
   LWX - Baltimore/Washington, Maryland/Virginia
   AKQ - Wakefield, Virginia
   MHX - Morehead City, North Carolina
   ILM - Wilmington, North Carolina
   CHS - Charleston, South Carolina

The present implementation (v1.1.0) adds the following 13 domains:

   SGX - San Diego, California
   LOX - Los Angeles, California
   MTR - Monterey,California
   EKA - Eureka, California
   MFR - Medford, Oregon
   PQR - Portland, Oregon
   SEW - Seattle, Washington
   HFO - Honolulu, Hawaii
   GUM - Tiyan, Guam
   AJK - Juneau, Alaska
   AER - Anchorage, Alaska
   ALU - Aleutian Islands, Alaska
   AFG - Fairbanks, Alaska

The NWPS is run 2-4 times per day, on demand, depending on the coastal Weather Forecast Office (WFO). The 3-hourly grids will be disseminated in GRIB2 format, as well as in graphical (png) format on http://polar.ncep.noaa.gov/waves/nwps/. Grid resolutions will be dependent upon individual coastal WFO. Each WFO will receive integral (total wave field) model guidance on a CG1 grid, and on up to 4 nested grids (CG2-CG5). In addition, partitioned wave fields will be available on a lower-resolution CG0 grid, if requested:

   CG0 grid - partition output on low-resolution version of overall computational domain CG1
   CG1 grid - integral output on overall computational grid
   CG2 grid - integral output on first nested grid, if applicable
   CG3 grid - integral output on second nested grid, if applicable
   CG4 grid - integral output on third nested grid, if applicable
   CG5 grid - integral output on fourth nested grid, if applicable

JOB DEPENDENCIES
This system has upstream dependencies on the following WCOSS models:
 - WW3 Multi_1 (wave boundary conditions)
 - RTOFS Global (surface current fields)
 - ESTOFS Atlantic and Pacific (water level fields)
 - MMAB Sea Ice Analysis (sea ice fields) 
 - P-Surge (water level fields)

The system has upstream data dependencies on:
 - User input received from AWIPS 2 v16.2.1 (and later) via LDM

This system currently has no downstream dependencies.

REQUIRED SYSTEM USAGE:

i) Input field preprocessing:
Preprocessing module to extract input fields from upstream models ESTOFS, Sea Ice, RTOFS and P-Surge to grids 
directly ingestible in NWPS. These run in parallel to the on-demand runs for each WFO (see below), and are staged 
for on-demand use in the latter.

ESTOFS water level and Sea Ice extraction:
CPU time:   3:17:02 h:min:sec
Max/Ave Memory :   199 MB / 57.86 MB
Num of CPUs :   1 (Serial, on 1 node)

RTOFS surface current extraction:
CPU time:   0:12:27 h:min:sec
Max/Ave Memory :   100 MB / 34.18 MB
Num of CPUs :   1 (Serial, on 1 node)

P-Surge water level extraction (tropical conditions in SR/ER):
CPU time :   2:59:20 h:min:sec
Max/Ave Memory :   27 MB / 22.08 MB
Num of CPUs :   23 (cfp parallel, distrib. over 2 nodes)

ii) Real-time, on-demand model:
For each of the 36 on-demand WFO runs, the following tasks are executed: PREP, FORECAST_CG1, POST_CG1, PRDGEN_CG1,  WAVETRACK_CG1, PRDGEN_CG0. Additionally, up to 4 nested domains are run, using the tasks FORECAST_CGn, POST_CGn and PRDGEN_CGn in a loop of n. The system usage for each of these tasks depends on the specific WFO domain, and number of nest grids. Example system usage values for WFO Medford (MFR) are:

JNWPS_PREP
   CPU time :   12:52 min:sec
   Max/Ave Memory :   78 MB / 38.74 MB
   Num of CPUs :   1 (Serial)

JNWPS_FORECAST_CG1
   CPU time :   13:45 min:sec
   Max/Ave Memory :   40 MB / 29.14 MB
   Num of CPUs :   8 (Parallel)

JNWPS_POST_CG1 & JNWPS_PRDGEN_CG1
   CPU time :   14:39 min:sec
   Max/Ave Memory :   206 MB / 50.85 MB
   Num of CPUs :   1 (Serial)

JNWPS_WAVETRACK_CG1 & JNWPS_PRDGEN_CG0
   CPU time :   5:01 min:sec
   Max/Ave Memory :   142 MB / 22.57 MB
   Num of CPUs :   8 (Parallel)

JNWPS_FORECAST_CGn  (for MFL)
   CPU time :   23:51 min:sec
    Max/Ave Memory :   40 MB / 25.32 MB
   Num of CPUs :   8 (Parallel)

JNWPS_POST_CGn & JNWPS_PRDGEN_CGn 
   CPU time :   15:23 min:sec
   Max/Ave Memory :   28 MB / 20.69 MB
   Num of CPUs :   Up to 4 (Parallel)

Estimated average system usage for each of the 36 WFOs: 8 cores and peak memory of 206 MB RAM. A total of 144 cores on WCOSS TO4 (Cray) should be reserved for the on-demand operation, preferably EXCLUSIVE. With this configuration, the total average run time per WFO is (values of current v1.0.0 implementation in parentheses to show impacts):

SR:                      ER:                      WR:                      PR:
BRO: 45 min (47)         CHS: 83 min (60)         SGX:  59 min             HFO: 181 min
CRP: 46 min (48)         ILM: 41 min (44)         LOX:  88 min             GUM: 65 min
HGX: 69 min (72)         MHX: 100 min (99)        MTR:  86 min
LCH: 73 min (67)         AKQ: 54 min (51)         EKA:  56 min             AR:
LIX: 87 min (87)         LWX: 29 min (32)         MFR:  88 min            AJK: 174 min
MOB: 76 min (81)         PHI: 68 min (64)         PQR:  73 min             AER: 137 min
TAE: 72 min (55)         OKX: 70 min (66)         SEW:  57 min             ALU: 78 min
TBW: 92 min (107)        BOX: 82 min (94)                                  AFG: 110 min
KEY: 82 min (103)        GYX: 62 min (59)
MFL: 113 min (119)       CAR: 57 min (62)
MLB: 64 min (72)
JAX: 67 min (69)
SJU: 90 min (103)

Disk storage: Total GRIB2 volume (36 WFOs, all domains) = 2.5 GB/cycle ~ 2 cycles/day. Total PNG volume =  1.5 GB/cycle (29,500 files) ~ 2 cycles /day. Request data to be archived for 5 days.

BENEFIT OF CHANGE: 
Expand NWPS system to reach national coverage (36 WFO doamins). Coastal WFOs in WR, PR and AR now provided with high-resolution coastal wave guidance not produced on WCOSS previously.

USER IMPACT STATEMENT:
Coastal WFOs in WR, PR and AR now provided with high-resolution coastal wave guidance. WFOs in SR and ER mostly unaffected in functionality and timing of products - see timing in previous section. Exceptions are LCH, TAE and CHS (latter due to additional nest requested by users).

TECHNICAL IMPACT STATEMENT:
As described above, new GRIB2 files will become available for 13 additional WFOs in WR, PR and AR. Coastal WFOs who choose to run the local workstation implementation of NWPS alongside the WCOSS implementation (AWIPS 2 v16.2.1 NWPS GUI option “Both”) will potentially have two sets of GRIB2 files in their AWIPS system. The locally-produced files will have to be renamed to avoid clashes. 

RISKS:
This on-demand system has two primary technical risks:
1. Dataflow: Each of the 36 coastal WFOs will submit a set of three files via LDM for their respective runs. The data volume of each submission will be up to 3 MB. There is a risk that there would be insufficient capacity on the LDM to relay this data. If this occurs, the NWPS runs would not initiate on WCOSS.
2. The NWPS will handle individual runs of up to 36 coastal WFOs, but typically not all concurrently. Projections suggest that 144 TO4 (Cray) compute cores (6 nodes) would provide sufficient capacity, but the actual operational loading will have to be assessed once NWPS is implemented.  

PROPOSED IMPLEMENTATION
DATE:  October 25, 2016
TIME: 00z

BUGFIXES
- Bugzilla ID 359: Error checking – Various err_chks added after ush/* and scripts/*
- Bugzilla regarding WFO Key West (May 2): Numerical instability – Added numerical limiter and checked error handling
- Changed output dir to com/nwps/prod/${REGION}.${PDY}/${WFO}
- Cleaned up redundant code/directories. Added doc/ directory
- Added module file with build environment
- Corrected the encoding of exception values in GRIB2 files (for AWIPS GFE/D2D display).
- Changed web graphics color scales from “breathing” to fixed.

IMPLEMENTATION INSTRUCTIONS

I) Checking out the code from EMC-SVN

Make the $NWROOT home directory for the model (“nwps” for example, in an appropriate "save" directory):
> mkdir nwps
> cd nwps

Check out the NWPS code from the following emc-svn tag: https://svnemc.ncep.noaa.gov/projects/nwps/tags/IT_NWPS_WCOSS_ver1.1.0-20160707
> svn checkout https://svnemc.ncep.noaa.gov/projects/nwps/tags/IT_NWPS_WCOSS_ver1.1.0-20160707

II) Building the executables

First define the path variable ${NWPSdir} in your profile file, which points to the base of the code checked out under (I) above.
> NWPSdir=$(pwd)

Next, change directory to ${NWPSdir}/sorc/, and execute the general NWPS install script. This single step will install the total package, including all binary compilations:

> cd ${NWPSdir}/sorc
> ./make_NWPS.sh

Once the compilations are done, all executables are moved to ${NWPSdir}/exec, and the system will be ready.

