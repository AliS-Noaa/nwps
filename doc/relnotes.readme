TITLE: 
Nearshore Wave Prediction System (NWPS) v1.2.0

CHANGE REQUEST CLASS: 
Major change

DESCRIPTION OF CHANGE:  
This upgrade includes the following major changes:
- Increase of the NWPS forecast length from 102 h to 144 h, and the output frequency from 3-hourly to 1-hourly.
- Transition from regular model grids to unstructured meshes for 10 WFOs (TBW, MFL, SJU, MHX, AKQ, OKX, BOX, CAR, SGX, HFO).
- Inclusion of a regression model for rip current guidance at the above-mentioned 10 unstructured mesh WFOs (experimental).
- Inclusion of a regression model for wave runup erosion/overwash guidance at 7 of the above-mentioned WFOs (all except HFO, SGX, SJU; experimental)
- Inclusion of a fail-over to GFS wind input in case of incomplete or erroneous GFE wind input being received from a WFO/AWIPS.
- Inclusion of retrostpective mode.

The NWPS is run 1-8 times per day, on demand, depending on the coastal Weather Forecast Office (WFO). The 1-hourly grids will be disseminated in GRIB2 format, as well as in graphical (png) format on http://polar.ncep.noaa.gov/nwps/. Grid resolutions will be dependent upon individual coastal WFO. Each WFO will receive integral (total wave field) model guidance on a CG1 grid, and on up to 4 nested grids (CG2-CG5). In addition, partitioned wave fields will be available on a lower-resolution CG0 grid, if requested:

   CG0 grid - partition output on low-resolution version of overall computational domain CG1
   CG1 grid - integral output on overall computational grid
   CG2 grid - integral output on first nested grid, if applicable
   CG3 grid - integral output on second nested grid, if applicable
   CG4 grid - integral output on third nested grid, if applicable
   CG5 grid - integral output on fourth nested grid, if applicable

In the case of the 10 new unstructured meshes, the results on the native unstructured meshes are interpolated on the above regular grids for AWIPS display. 

JOB DEPENDENCIES
This system has upstream dependencies on the following WCOSS models:
 - WW3 Multi_1 (wave boundary conditions)
 - RTOFS Global (surface current fields)
 - ESTOFS Atlantic and Pacific (water level fields)
 - MMAB Sea Ice Analysis (sea ice fields) 
 - P-Surge (water level fields)
 - New dependency on GFS pgrb files (U10 fields for GFS fail-over option) 
Note that the dependency on the phased out WW3 Multi_2 has been removed.

The system has upstream data dependencies on:
 - User input received from AWIPS via LDM

This system currently has no downstream dependencies.

REQUIRED SYSTEM USAGE:

i) Input field preprocessing:
Preprocessing module to extract input fields from upstream models ESTOFS, Sea Ice, RTOFS and P-Surge to grids 
directly ingestible in NWPS. These run in parallel to the on-demand runs for each WFO (see below), and are staged 
for on-demand use in the latter.

ESTOFS water level and Sea Ice extraction:
   CPU time:   1:09:32 h:min:sec
   Max/Ave Memory :   28 MB / 21.99 MB
   Num of CPUs :   36 (cfp parallel, distrib. over 2 nodes)

RTOFS surface current extraction:
   CPU time:   0:16:05 h:min:sec
   Max/Ave Memory :   134 MB / 40.70 MB
   Num of CPUs :   1 (Serial, on 1 node)

P-Surge water level extraction (tropical conditions in SR/ER):
   CPU time :   2:59:20 h:min:sec
   Max/Ave Memory :   27 MB / 22.08 MB
   Num of CPUs :   23 (cfp parallel, distrib. over 2 nodes)

ii) Real-time, on-demand model:
For each of the 36 on-demand WFO runs, the following tasks are executed: PREP, FORECAST_CG1, POST_CG1, PRDGEN_CG1,  WAVETRACK_CG1, PRDGEN_CG0. Additionally, up to 4 nested domains are run, using the tasks FORECAST_CGn, POST_CGn and PRDGEN_CGn in a loop of n. The system usage for each of these tasks depends on the specific WFO domain, and number of nest grids. Example system usage values for the new unstructured-mesh WFO Upton (OKX) are:

JNWPS_PREP
   CPU time :   06:00 min:sec
   Max/Ave Memory :   80 MB / 41 MB
   Num of CPUs :   1 (Serial)

JNWPS_FORECAST_CG1
   CPU time :   28:43 min:sec
   Max/Ave Memory :   40 MB / 32.22 MB
   Num of CPUs :   48 (Parallel)

JNWPS_POST_CG1 & JNWPS_PRDGEN_CG1
   CPU time :   11:29 min:sec
   Max/Ave Memory :   2440 MB / 82.82 MB
   Num of CPUs :   11 (Parallel)

JNWPS_WAVETRACK_CG1 & JNWPS_PRDGEN_CG0
   CPU time :   2:57 min:sec
   Max/Ave Memory :   229 MB / 22.81 MB
   Num of CPUs :   8 (Parallel)

JNWPS_FORECAST_CGn
   CPU time :   02:01 min:sec
    Max/Ave Memory :   40 MB / 20.83 MB
   Num of CPUs :   10 (Parallel)

JNWPS_POST_CGn & JNWPS_PRDGEN_CGn 
   CPU time :   8:02 min:sec
   Max/Ave Memory :   27 MB / 20.62 MB
   Num of CPUs :   Up to 4 (Parallel)

Estimated average system usage for each of the 36 WFOs: 16-48 cores, depending on WFO domain, and peak memory of 2440 MB RAM. A total of 32 nodes on WCOSS TO4 (Cray) should be reserved for the on-demand operation, preferably EXCLUSIVE (up from 18 nodes in v1.1). With this configuration, the total average run time per WFO is (values of current v1.1.0 implementation in parentheses to show impacts):

SR:                      ER:                      WR:                           PR:
BRO: 32 min (45)         CHS: 69 min (83)         SGX:  67 min (59 min)         HFO: 125 min (181 min)
CRP: 34 min (46)         ILM: 29 min (41)         LOX:  75 min (88 min)         GUM: 45 min (65 min)
HGX: 70 min (69)         MHX: 90 min (100)        MTR:  82 min (86 min)
LCH: 37 min (73)         AKQ: 58 min (54)         EKA:  45 min (56 min)         AR:
LIX: 76 min (87)         LWX: 15 min (29)         MFR:  76 min (88 min)         AJK: 139 min (174 min)
MOB: 68 min (76)         PHI: 53 min (68)         PQR:  60 min (73 min)         AER: 135 min (137 min)
TAE: 67 min (72)         OKX: 75 min (70)         SEW:  57 min (57 min)         ALU: 47 min (78 min)
TBW: 55 min (92)         BOX: 82 min (82)                                       AFG: 102 min (110 min)
KEY: 86 min (82)         GYX: 67 min (62)
MFL: 91 min (113)        CAR: 59 min (57)
MLB: 68 min (64)
JAX: 60 min (67)
SJU: 80 min (90)

Disk storage: Total GRIB2 volume (36 WFOs, all domains) = 16.6 GB/day on ptmp. Request data to be stored for 5 days.

HPSS tape storage: Total volume (36 WFOs, all domains) = 82.2 GB/day, incl. space of retrospective inputs (up from 18.2 GB/day in v1.1). Request data to be archived for 2 years.

BENEFIT OF CHANGE: 
Meet the needs of coastal WFOs for longer-term, higher-frequency nearshore wave and coastal hazard guidance. Improved overall system robustness from the inclusion of GFS fail-over.

USER IMPACT STATEMENT:
All coastal WFOs now provided with 144 h, 1-hourly high-resolution coastal wave guidance (increased from 102 h, 3-hourly). GRIB2 grid resolutions remain unchanged, also for WFOs now running on unstructured meshes. Experimental rip current guidance at 10 WFOs (TBW, MFL, SJU, MHX, AKQ, OKX, BOX, CAR, SGX, HFO), for validation purposes. Experimental erosion/overwash guidance at 7 WFOs (TBW, MFL, MHX, AKQ, OKX, BOX, CAR), for validation purposes. Run times at or below current v1.1 times. Only WFO SGX has a delay of more than 5 min (at 8 min), due to extended output and additional guidance products. GFS fail-over will now be invoked when the received GFE wind input file is either: (a) absent, (b) incomplete (less than 144 h of data), or (c) spurious (contains values in excess of 199 knots). Normal operation will resume again as soon as a correct GFE wind file is received.

TECHNICAL IMPACT STATEMENT:
AWIPS GFE/D2D will need to be updated to v17.3.1 in order to display the extended forecast hours (out to 144 h). WMO headers and GRIB2 tables have been updated for this purpose.  

RISKS:
This on-demand system has two primary technical risks:
1. Dataflow: Each of the 36 coastal WFOs will submit a set of three files via LDM for their respective runs. The data volume of each submission will be up to 3 MB. There is a risk that there would be insufficient capacity on the LDM to relay this data. If this occurs, the NWPS runs would not initiate on WCOSS.
2. The NWPS will handle individual runs of up to 36 coastal WFOs, but typically not all concurrently. Projections suggest that 32 reserved compute nodes (2 for each of 18 concurrently-running WFO slots) would provide sufficient capacity.  

PROPOSED IMPLEMENTATION
DATE:  October 31, 2017
TIME: 00z

BUGFIXES
- Bug ID 505: nwps_post job can not rerun by itself
- Bug ID 510: Clean up the symbolic links in nwps basemap library
- Bug ID 517: Post-CG1 failure, large values in wind field
- Bug ID 530: Improved error code checking (Python plotting scripts)
- Bug ID 531: geoslib compilation
- Bug ID 543: Remove the cycle dependency to date command in nwps_prep job

IMPLEMENTATION INSTRUCTIONS

I) Checking out the code from EMC-SVN

Make the $NWROOT home directory for the model (“nwps” for example, in an appropriate "save" directory):
> mkdir nwps
> cd nwps

Check out the NWPS code from the following emc-svn tag: https://svnemc.ncep.noaa.gov/projects/nwps/tags/IT_NWPS_WCOSS_ver1.2.0-20170717
> svn checkout https://svnemc.ncep.noaa.gov/projects/nwps/tags/IT_NWPS_WCOSS_ver1.2.0-20170717

II) Building the executables

First define the path variable ${NWPSdir} in your profile file, which points to the base of the code checked out under (I) above.
> NWPSdir=$(pwd)

Next, change directory to ${NWPSdir}/sorc/, and execute the general NWPS install script. This single step will install the total package, including all binary compilations:

> cd ${NWPSdir}/sorc
> ./make_NWPS.sh

Once the compilations are done, all executables are moved to ${NWPSdir}/exec, and the system will be ready.

